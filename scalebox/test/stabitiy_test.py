#!/usr/bin/env python3
"""
ç¨³å®šæ€§æµ‹è¯•è„šæœ¬ - å¹¶å‘æ‰§è¡ŒCodeInterpreteréªŒè¯æµ‹è¯•
"""

import concurrent.futures
import time
import json
import logging
import threading
from typing import List, Dict, Any
import sys
import argparse

# å¯¼å…¥åŸå§‹æµ‹è¯•ä»£ç 
from code_interpreter_validator import CodeInterpreterValidator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(threadName)s] - %(message)s'
)
logger = logging.getLogger(__name__)


class StabilityTester:
    """ç¨³å®šæ€§æµ‹è¯•å™¨"""

    def __init__(self, concurrency: int = 10):
        self.concurrency = concurrency
        self.results = []
        self.lock = threading.Lock()
        self.test_counter = 0
        self.total_tests = 0

    def get_test_methods(self) -> List[str]:
        """è·å–æ‰€æœ‰æµ‹è¯•æ–¹æ³•"""
        validator = CodeInterpreterValidator()
        test_methods = []

        # è·å–æ‰€æœ‰ä»¥test_å¼€å¤´çš„æ–¹æ³•
        for method_name in dir(validator):
            if method_name.startswith('test_') and callable(getattr(validator, method_name)):
                test_methods.append(method_name)

        self.total_tests = len(test_methods)
        logger.info(f"å‘ç° {self.total_tests} ä¸ªæµ‹è¯•æ–¹æ³•")
        return test_methods

    def run_single_test(self, test_name: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        thread_name = threading.current_thread().name
        test_id = 0

        with self.lock:
            self.test_counter += 1
            test_id = self.test_counter

        logger.info(f"[çº¿ç¨‹ {thread_name}] å¼€å§‹æ‰§è¡Œæµ‹è¯• {test_id}/{self.total_tests}: {test_name}")

        start_time = time.time()
        success = False
        error_message = ""
        duration = 0

        try:
            # ä¸ºæ¯ä¸ªæµ‹è¯•åˆ›å»ºç‹¬ç«‹çš„éªŒè¯å™¨å®ä¾‹
            validator = CodeInterpreterValidator()

            # è¿è¡Œæ²™ç®±åˆ›å»ºæµ‹è¯•
            validator.test_code_interpreter_creation()

            # è¿è¡Œç›®æ ‡æµ‹è¯•
            test_method = getattr(validator, test_name)
            test_method()

            duration = time.time() - start_time
            success = True
            logger.info(f"[çº¿ç¨‹ {thread_name}] âœ… æµ‹è¯•é€šè¿‡: {test_name} ({duration:.3f}s)")

        except Exception as e:
            duration = time.time() - start_time
            error_message = str(e)
            logger.error(f"[çº¿ç¨‹ {thread_name}] âŒ æµ‹è¯•å¤±è´¥: {test_name} - {error_message} ({duration:.3f}s)")

        finally:
            # æ¸…ç†èµ„æº
            try:
                if 'validator' in locals():
                    validator.cleanup()
            except Exception as cleanup_error:
                logger.warning(f"[çº¿ç¨‹ {thread_name}] æ¸…ç†èµ„æºæ—¶å‡ºé”™: {cleanup_error}")

        result = {
            'test_id': test_id,
            'test_name': test_name,
            'thread_name': thread_name,
            'success': success,
            'error_message': error_message,
            'duration': duration,
            'timestamp': time.time()
        }

        with self.lock:
            self.results.append(result)

        return result

    def run_concurrent_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        test_methods = self.get_test_methods()

        if not test_methods:
            logger.error("æœªå‘ç°æµ‹è¯•æ–¹æ³•")
            return {}

        logger.info(f"å¼€å§‹ç¨³å®šæ€§æµ‹è¯•ï¼Œå¹¶å‘æ•°: {self.concurrency}")
        logger.info(f"æ€»æµ‹è¯•æ•°: {len(test_methods)}")

        start_time = time.time()

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘æµ‹è¯•
        with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.concurrency,
                thread_name_prefix='TestWorker'
        ) as executor:

            # æäº¤æ‰€æœ‰æµ‹è¯•ä»»åŠ¡
            future_to_test = {
                executor.submit(self.run_single_test, test_name): test_name
                for test_name in test_methods
            }

            # ç­‰å¾…æ‰€æœ‰æµ‹è¯•å®Œæˆ
            completed = 0
            for future in concurrent.futures.as_completed(future_to_test):
                test_name = future_to_test[future]
                try:
                    future.result()
                except Exception as exc:
                    logger.error(f"æµ‹è¯• {test_name} ç”Ÿæˆå¼‚å¸¸: {exc}")
                completed += 1
                logger.info(f"æµ‹è¯•è¿›åº¦: {completed}/{len(test_methods)}")

        total_duration = time.time() - start_time

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = self.generate_report(total_duration)

        return report

    def generate_report(self, total_duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        successful_tests = [r for r in self.results if r['success']]
        failed_tests = [r for r in self.results if not r['success']]

        total_tests = len(self.results)
        success_count = len(successful_tests)
        failure_count = len(failed_tests)
        success_rate = (success_count / total_tests * 100) if total_tests > 0 else 0

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        durations = [r['duration'] for r in self.results]
        avg_duration = sum(durations) / len(durations) if durations else 0
        max_duration = max(durations) if durations else 0
        min_duration = min(durations) if durations else 0

        report = {
            'summary': {
                'total_tests': total_tests,
                'successful_tests': success_count,
                'failed_tests': failure_count,
                'success_rate': round(success_rate, 2),
                'total_duration': round(total_duration, 3),
                'concurrency': self.concurrency,
                'avg_duration_per_test': round(avg_duration, 3),
                'max_duration': round(max_duration, 3),
                'min_duration': round(min_duration, 3)
            },
            'successful_tests': [
                {
                    'test_name': r['test_name'],
                    'duration': round(r['duration'], 3),
                    'thread': r['thread_name']
                } for r in successful_tests
            ],
            'failed_tests': [
                {
                    'test_name': r['test_name'],
                    'error': r['error_message'],
                    'duration': round(r['duration'], 3),
                    'thread': r['thread_name']
                } for r in failed_tests
            ],
            'execution_timeline': [
                {
                    'test_id': r['test_id'],
                    'test_name': r['test_name'],
                    'thread': r['thread_name'],
                    'success': r['success'],
                    'duration': round(r['duration'], 3),
                    'timestamp': r['timestamp']
                } for r in self.results
            ]
        }

        return report

    def print_detailed_report(self, report: Dict[str, Any]):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š"""
        summary = report['summary']

        print("\n" + "=" * 80)
        print("ğŸš€ CODEINTERPRETER ç¨³å®šæ€§æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)

        print(f"\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
        print(f"   æ€»æµ‹è¯•æ•°:     {summary['total_tests']}")
        print(f"   é€šè¿‡æµ‹è¯•:     {summary['successful_tests']} âœ…")
        print(f"   å¤±è´¥æµ‹è¯•:     {summary['failed_tests']} âŒ")
        print(f"   æˆåŠŸç‡:       {summary['success_rate']}%")
        print(f"   æ€»æ‰§è¡Œæ—¶é—´:   {summary['total_duration']}s")
        print(f"   å¹¶å‘æ•°:       {summary['concurrency']}")
        print(f"   å¹³å‡æµ‹è¯•æ—¶é—´: {summary['avg_duration_per_test']}s")
        print(f"   æœ€é•¿æµ‹è¯•æ—¶é—´: {summary['max_duration']}s")
        print(f"   æœ€çŸ­æµ‹è¯•æ—¶é—´: {summary['min_duration']}s")

        # æ‰“å°æˆåŠŸæµ‹è¯•
        if report['successful_tests']:
            print(f"\nâœ… é€šè¿‡çš„æµ‹è¯• ({len(report['successful_tests'])}):")
            for test in report['successful_tests']:
                print(f"   - {test['test_name']} ({test['duration']}s) [{test['thread']}]")

        # æ‰“å°å¤±è´¥æµ‹è¯•
        if report['failed_tests']:
            print(f"\nâŒ å¤±è´¥çš„æµ‹è¯• ({len(report['failed_tests'])}):")
            for test in report['failed_tests']:
                print(f"   - {test['test_name']}")
                print(f"     é”™è¯¯: {test['error']}")
                print(f"     æ—¶é—´: {test['duration']}s")
                print(f"     çº¿ç¨‹: {test['thread']}")

        # æ‰“å°æ‰§è¡Œæ—¶é—´çº¿
        print(f"\nâ° æ‰§è¡Œæ—¶é—´çº¿:")
        for execution in sorted(report['execution_timeline'], key=lambda x: x['timestamp']):
            status = "âœ…" if execution['success'] else "âŒ"
            print(f"   {status} [{execution['thread']}] {execution['test_name']} ({execution['duration']}s)")

        print("\n" + "=" * 80)

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"stability_test_report_{timestamp}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {filename}")
        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='CodeInterpreterç¨³å®šæ€§æµ‹è¯•')
    parser.add_argument(
        '--concurrency',
        type=int,
        default=10,
        help='å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 10)'
    )
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)'
    )

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.getLogger().setLevel(getattr(logging, args.log_level))

    logger.info(f"å¯åŠ¨ç¨³å®šæ€§æµ‹è¯•ï¼Œå¹¶å‘æ•°: {args.concurrency}")

    tester = StabilityTester(concurrency=args.concurrency)

    try:
        report = tester.run_concurrent_tests()
        tester.print_detailed_report(report)

        # æ ¹æ®æˆåŠŸç‡è¿”å›é€‚å½“çš„é€€å‡ºç 
        success_rate = report['summary']['success_rate']
        if success_rate >= 95:
            logger.info(f"ğŸ‰ æµ‹è¯•æˆåŠŸ! æˆåŠŸç‡: {success_rate}%")
            sys.exit(0)
        elif success_rate >= 80:
            logger.warning(f"âš ï¸  æµ‹è¯•åŸºæœ¬é€šè¿‡ï¼Œä½†æœ‰æ”¹è¿›ç©ºé—´ã€‚æˆåŠŸç‡: {success_rate}%")
            sys.exit(0)
        else:
            logger.error(f"ğŸ’¥ æµ‹è¯•å¤±è´¥! æˆåŠŸç‡: {success_rate}%")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()